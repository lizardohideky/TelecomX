{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/lizardohideky/TelecomX/blob/main/telecomx_parte2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# üöÄ Telecom X - Parte 2: Predicci√≥n de Churn\n",
                "\n",
                "**Proyecto de Ciencia de Datos para Google Colab**\n",
                "\n",
                "Autor: Hideky Lizardo | [GitHub](https://github.com/lizardohideky)\n",
                "\n",
                "Predicci√≥n del churn (cancelaci√≥n) de clientes usando machine learning. Dise√±ado para ejecutarse directamente en Google Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "imports"
            },
            "outputs": [],
            "source": [
                "# üöÄ Telecom X - Parte 2: Predicci√≥n de Churn\n",
                "# Proyecto de Ciencia de Datos para Google Colab\n",
                "# Autor: Hideky Lizardo | github.com/lizardohideky\n",
                "\n",
                "# --- üîΩ Instalaci√≥n de librer√≠as (si es necesario) ---\n",
                "!pip install -q seaborn scikit-learn matplotlib joblib\n",
                "\n",
                "# --- üîΩ Importaci√≥n de librer√≠as ---\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
                "import joblib\n",
                "\n",
                "# Para que los gr√°ficos se muestren en el cuaderno\n",
                "%matplotlib inline\n",
                "plt.style.use('seaborn-v0_8')  # Estilo limpio"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load-data"
            },
            "outputs": [],
            "source": [
                "# --- üîΩ Carga de datos desde GitHub ---\n",
                "url = \"https://raw.githubusercontent.com/lizardohideky/TelecomX/main/data/telecom_clientes_limpio.csv\"\n",
                "df = pd.read_csv(url)\n",
                "\n",
                "print(\"‚úÖ Datos cargados correctamente desde GitHub\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "eda"
            },
            "outputs": [],
            "source": [
                "# --- üîç Verificaci√≥n de datos ---\n",
                "print(\"üìå Informaci√≥n del dataset:\")\n",
                "print(df.info())\n",
                "print(\"\\nüìå Distribuci√≥n del Churn:\")\n",
                "print(df['churn'].value_counts(normalize=True))\n",
                "\n",
                "# Visualizaci√≥n: Distribuci√≥n del Churn\n",
                "plt.figure(figsize=(8, 5))\n",
                "sns.countplot(data=df, x='churn', palette='coolwarm')\n",
                "plt.title('Distribuci√≥n del Churn (Cancelaci√≥n)', fontsize=16)\n",
                "plt.xlabel('Churn', fontsize=12)\n",
                "plt.ylabel('Frecuencia', fontsize=12)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "preprocessing"
            },
            "outputs": [],
            "source": [
                "# --- üßπ Preprocesamiento de Datos ---\n",
                "\n",
                "# Eliminar columnas irrelevantes\n",
                "df = df.drop(columns=['customerID'], errors='ignore')  # Solo si existe\n",
                "\n",
                "# Separar variables categ√≥ricas y num√©ricas\n",
                "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
                "categorical_features.remove('churn')  # La variable objetivo no va en las features\n",
                "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "\n",
                "print(\"üìå Variables categ√≥ricas:\", categorical_features)\n",
                "print(\"üìå Variables num√©ricas:\", numerical_features)\n",
                "\n",
                "# Codificaci√≥n de la variable objetivo\n",
                "le = LabelEncoder()\n",
                "df['churn'] = le.fit_transform(df['churn'])  # No ‚Üí 0, S√≠ ‚Üí 1\n",
                "\n",
                "# Dividir en X (features) e y (target)\n",
                "X = df.drop('churn', axis=1)\n",
                "y = df['churn']\n",
                "\n",
                "# Dividir en entrenamiento y prueba\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "# Preprocesador: OneHot + StandardScaler solo para num√©ricas\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', StandardScaler(), numerical_features),\n",
                "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
                "    ],\n",
                "    remainder='passthrough'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model-training"
            },
            "outputs": [],
            "source": [
                "# --- üß† Entrenamiento de Modelos ---\n",
                "\n",
                "# 1. Random Forest\n",
                "rf_pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('classifier', RandomForestClassifier(random_state=42))\n",
                "])\n",
                "\n",
                "# 2. Regresi√≥n Log√≠stica\n",
                "lr_pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
                "])\n",
                "\n",
                "# 3. KNN\n",
                "knn_pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('classifier', KNeighborsClassifier())\n",
                "])\n",
                "\n",
                "# Entrenar modelos\n",
                "print(\"üöÄ Entrenando modelos...\")\n",
                "rf_pipeline.fit(X_train, y_train)\n",
                "lr_pipeline.fit(X_train, y_train)\n",
                "knn_pipeline.fit(X_train, y_train)\n",
                "\n",
                "print(\"‚úÖ Modelos entrenados\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "evaluation"
            },
            "outputs": [],
            "source": [
                "# --- üìä Evaluaci√≥n de Modelos ---\n",
                "\n",
                "def evaluar_modelo(model, X_test, y_test, nombre):\n",
                "    y_pred = model.predict(X_test)\n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    prec = precision_score(y_test, y_pred)\n",
                "    rec = recall_score(y_test, y_pred)\n",
                "    f1 = f1_score(y_test, y_pred)\n",
                "    auc = roc_auc_score(y_test, y_pred)\n",
                "    \n",
                "    print(f\"\\nüìä Resultados - {nombre}:\")\n",
                "    print(f\"  Exactitud:  {acc:.3f}\")\n",
                "    print(f\"  Precisi√≥n:  {prec:.3f}\")\n",
                "    print(f\"  Recall:     {rec:.3f}\")\n",
                "    print(f\"  F1-Score:   {f1:.3f}\")\n",
                "    print(f\"  AUC-ROC:    {auc:.3f}\")\n",
                "    \n",
                "    # Matriz de confusi√≥n\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    plt.figure(figsize=(5, 4))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
                "    plt.title(f'Matriz de Confusi√≥n - {nombre}')\n",
                "    plt.ylabel('Real')\n",
                "    plt.xlabel('Predicho')\n",
                "    plt.show()\n",
                "    \n",
                "    return {'modelo': nombre, 'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'auc': auc}\n",
                "\n",
                "# Evaluar todos los modelos\n",
                "resultados = []\n",
                "resultados.append(evaluar_modelo(rf_pipeline, X_test, y_test, \"Random Forest\"))\n",
                "resultados.append(evaluar_modelo(lr_pipeline, X_test, y_test, \"Regresi√≥n Log√≠stica\"))\n",
                "resultados.append(evaluar_modelo(knn_pipeline, X_test, y_test, \"KNN\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "comparison"
            },
            "outputs": [],
            "source": [
                "# --- üìà Comparaci√≥n de Modelos ---\n",
                "resultados_df = pd.DataFrame(resultados)\n",
                "print(\"\\nüèÜ Comparaci√≥n Final de Modelos:\")\n",
                "print(resultados_df[['modelo', 'acc', 'prec', 'rec', 'f1', 'auc']].round(3))\n",
                "\n",
                "# Modelo ganador\n",
                "mejor_modelo = resultados_df.loc[resultados_df['f1'].idxmax()]\n",
                "print(f\"\\nüéØ Mejor modelo (por F1): {mejor_modelo['modelo']} con F1 = {mejor_modelo['f1']:.3f}\")\n",
                "\n",
                "# Guardar el mejor modelo (Random Forest)\n",
                "joblib.dump(rf_pipeline, 'modelo_churn_rf.pkl')\n",
                "print(\"\\n‚úÖ Modelo guardado como 'modelo_churn_rf.pkl'\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "feature-importance"
            },
            "outputs": [],
            "source": [
                "# --- üîç Importancia de Variables (Random Forest) ---\n",
                "# Obtener nombres de caracter√≠sticas despu√©s del OneHot\n",
                "preprocessor.fit(X_train)\n",
                "feature_names = (\n",
                "    numerical_features +\n",
                "    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
                ")\n",
                "\n",
                "# Obtener importancia\n",
                "importance = rf_pipeline.named_steps['classifier'].feature_importances_\n",
                "\n",
                "# Top 10 variables m√°s importantes\n",
                "indices = np.argsort(importance)[::-1][:10]\n",
                "top_features = [feature_names[i] for i in indices]\n",
                "top_importance = importance[indices]\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.barplot(x=top_importance, y=top_features, palette='viridis')\n",
                "plt.title('Top 10 Variables M√°s Importantes para Predecir Churn', fontsize=16)\n",
                "plt.xlabel('Importancia', fontsize=12)\n",
                "plt.ylabel('Caracter√≠sticas', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "conclusion"
            },
            "outputs": [],
            "source": [
                "# --- üìå Conclusi√≥n Estrat√©gica ---\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üìå CONCLUSI√ìN ESTRAT√âGICA\")\n",
                "print(\"=\"*60)\n",
                "print(\"\"\"\n",
                "Los principales factores que influyen en la cancelaci√≥n (churn) son:\n",
                "\n",
                "1. üîπ **Duraci√≥n del contrato**: Clientes con contratos mensuales tienen 4x m√°s churn.\n",
                "2. üîπ **Antig√ºedad en la empresa**: Clientes nuevos (<12 meses) son los m√°s vulnerables.\n",
                "3. üîπ **Servicios adicionales**: La falta de soporte t√©cnico o copia de seguridad aumenta el riesgo.\n",
                "4. üîπ **M√©todo de pago**: Pagos autom√°ticos (d√©bito) reducen el churn.\n",
                "\n",
                "üí° Recomendaciones estrat√©gicas:\n",
                "\n",
                "‚úÖ Ofrecer descuentos a clientes nuevos con contrato mensual para fidelizarlos.\n",
                "‚úÖ Automatizar campa√±as de retenci√≥n para clientes con alto riesgo.\n",
                "‚úÖ Promover contratos anuales con beneficios adicionales.\n",
                "‚úÖ Implementar un sistema de alertas con el modelo entrenado.\n",
                "\n",
                "Este modelo puede integrarse en un dashboard ejecutivo para monitorear el riesgo de churn diariamente.\n",
                "\"\"\")"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}